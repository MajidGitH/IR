{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'version' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4bc6a30f992c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'version' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a4637bbefc43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpython\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-b8041cebc60a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-b8041cebc60a>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    Press Shift + Tab to navigate to chat history.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    import numpy\n",
    "    numpy.version.version\n",
    "\n",
    "Press Shift + Tab to navigate to chat history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    import numpy\n",
    "    numpy.version.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Tokenization is the process of breaking down text document apart into those pieces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the process of breaking down text document apart into those pieces\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim as gs #تقطيع الجملة لكلمات\n",
    "tokenizedWord= list(gs.utils.tokenize(sentence)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'document', 'apart', 'into', 'those', 'pieces']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizedWord) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tokenize in module gensim.utils:\n",
      "\n",
      "tokenize(text, lowercase=False, deacc=False, encoding='utf8', errors='strict', to_lower=False, lower=False)\n",
      "    Iteratively yield tokens as unicode strings, optionally removing accent marks and lowercasing it.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    text : str or bytes\n",
      "        Input string.\n",
      "    deacc : bool, optional\n",
      "        Remove accentuation using :func:`~gensim.utils.deaccent`?\n",
      "    encoding : str, optional\n",
      "        Encoding of input string, used as parameter for :func:`~gensim.utils.to_unicode`.\n",
      "    errors : str, optional\n",
      "        Error handling behaviour, used as parameter for :func:`~gensim.utils.to_unicode`.\n",
      "    lowercase : bool, optional\n",
      "        Lowercase the input string?\n",
      "    to_lower : bool, optional\n",
      "        Same as `lowercase`. Convenience alias.\n",
      "    lower : bool, optional\n",
      "        Same as `lowercase`. Convenience alias.\n",
      "    \n",
      "    Yields\n",
      "    ------\n",
      "    str\n",
      "        Contiguous sequences of alphabetic characters (no digits!), using :func:`~gensim.utils.simple_tokenize`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.utils import tokenize\n",
      "        >>> list(tokenize('Nic nemůže letět rychlostí vyšší, než 300 tisíc kilometrů za sekundu!', deacc=True))\n",
      "        [u'Nic', u'nemuze', u'letet', u'rychlosti', u'vyssi', u'nez', u'tisic', u'kilometru', u'za', u'sekundu']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gs.utils.tokenize) # يساعدك تفهم كل ميثود ايش تسوي"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 3), (26, 1), (27, 1), (28, 1), (29, 3), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 2), (45, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "text = [\"\"\" In computer science, artificial\n",
    "intelligence (AI), sometimes called machine\n",
    "intelligence, is intelligence demonstrated by\n",
    "machines, in contrast to the natural intelligence\n",
    "displayed by humans and animals. Computer science\n",
    "defines AI research as the study of intelligent\n",
    "agents: any device that perceives its environment and\n",
    "takes actions that maximize its chance of successfully\n",
    "achieving its goals\"\"\"]\n",
    "tokens = [[token for token in sentence.split()] for sentence\n",
    "in text]\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token,\n",
    "allow_update=True) for token in tokens]\n",
    "print(gensim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('(AI),', 1), ('AI', 1), ('Computer', 1), ('In', 1), ('achieving', 1), ('actions', 1), ('agents:', 1), ('and', 2), ('animals.', 1), ('any', 1), ('artificial', 1), ('as', 1), ('by', 2), ('called', 1), ('chance', 1), ('computer', 1), ('contrast', 1), ('defines', 1), ('demonstrated', 1), ('device', 1), ('displayed', 1), ('environment', 1), ('goals', 1), ('humans', 1), ('in', 1), ('intelligence', 3), ('intelligence,', 1), ('intelligent', 1), ('is', 1), ('its', 3), ('machine', 1), ('machines,', 1), ('maximize', 1), ('natural', 1), ('of', 2), ('perceives', 1), ('research', 1), ('science', 1), ('science,', 1), ('sometimes', 1), ('study', 1), ('successfully', 1), ('takes', 1), ('that', 2), ('the', 2), ('to', 1)]]\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = [[(gensim_dictionary[id], frequence) for id, frequence in couple] for couple in gensim_corpus]\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\file.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-40bfe9df37a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m tokens = [simple_preprocess(sentence, deacc=True) \n\u001b[1;32m----> 6\u001b[1;33m           for sentence in open(r'C:\\Users\\ASUS\\file.txt',\n\u001b[0m\u001b[0;32m      7\u001b[0m                                encoding='utf-8')]\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ASUS\\\\file.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "tokens = [simple_preprocess(sentence, deacc=True) \n",
    "          for sentence in open(r'C:\\Users\\ASUS\\file.txt',\n",
    "                               encoding='utf-8')]\n",
    "\n",
    "gensim_dictionary = corpora.Dictionary()\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token, allow_update=True)\n",
    "                 for token in tokens]\n",
    "word_frequencies = [[(gensim_dictionary[id], frequence) \n",
    "                     for id, frequence in couple] \n",
    "                    for couple in gensim_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
